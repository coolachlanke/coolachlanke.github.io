<!-- /projects/neuralnet/index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Scratch-built Neural Network</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- Bootstrap CSS -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
  <link href="../../style/custom.css" rel="stylesheet">
  <link rel="stylesheet" href="neuralnet.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css">

  <!-- MathJax (equations) -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Favicons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="apple-touch-icon" sizes="180x180" href="../../images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../images/favicon/favicon-32x32.png">
  <link rel="manifest" href="../../images/favicon/site.webmanifest">

  <!-- Analytics -->
  <script defer data-domain="lachlancooke.com" src="https://plausible.io/js/script.js"></script>
</head>

<body>
  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg navbar-light">
    <div class="container-fluid">
      <a class="navbar-brand text-dark" href="/">
        <img src="../../images/lc_site_lgooo.png" alt="LC" style="height: 48px; margin-right: 10px;">
        Lachlan Cooke
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse"
              data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
              aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item"><a class="nav-link text-dark" href="/">Home</a></li>
          <li class="nav-item"><a class="nav-link text-dark" href="/projects">Projects</a></li>
          <li class="nav-item"><a class="nav-link text-dark" href="/about">About</a></li>
          <li class="nav-item"><a class="nav-link text-dark" href="/contact">Contact</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Main -->
  <main id="top" class="container" style="max-width:100ch; margin:0 auto;">
    <!-- Title -->
    <div class="text-center mt-5 mb-4">
      <h1 class="display-4 font-weight-bolder">Scratch-built Neural Network</h1>
      <p class="lead">Neural network designed to classify MNIST handwritten digits, built from raw linear algebra and calculus.</p>
    </div>

    <!-- Interactive Model Canvas -->
    <div class="d-flex flex-column align-items-center mt-4 mb-5">
      <canvas id="mnist-canvas" width="28" height="28"></canvas>

      <div class="d-flex justify-content-center mt-3">
        <button onclick="clearCanvas()" class="btn btn-outline-dark" style="margin-right: 1rem;">Clear</button>
        <button onclick="predictImage()" class="btn btn-outline-primary">Predict</button>
      </div>
      <!-- Toggle checkbox below -->
      <div class="form-check form-switch mt-3 text-center">
        <input class="form-check-input" type="checkbox" id="toggleMatrix" onchange="toggleMatrixVisibility()">
        <label class="form-check-label" for="toggleMatrix">Show Matrix</label>
      </div>
    </div>
    

    <!-- Matrix wrapper (initially hidden) -->
    <div id="matrix-wrapper" style="display: none; justify-content: center;">
      <div id="latex-matrix" class="math-latex-scale"></div>
    </div>


    <!-- Table of Contents -->
    <div class="toc-list-container mt-5">
      <div class="card toc-list">
        <div class="card-header">
          <h2>Contents</h2>
        </div>
        <div class="list-group list-group-flush">
          <a href="#introduction" class="list-group-item list-group-item-action">1. Introduction</a>
          <a href="#handwrittenmnist" class="list-group-item list-group-item-action">2. Handwritten Digits and the MNIST Set</a>
          <a href="#neural-network-architecture" class="list-group-item list-group-item-action">3. Neural Network Architecture</a>
          <a href="#forward-propagation-loss" class="list-group-item list-group-item-action">4. Forward Propagation and Loss</a>
          <a href="#backpropagation-from-scratch" class="list-group-item list-group-item-action">5. Backpropagation from Scratch</a>
          <a href="#training-loop-and-optimisation" class="list-group-item list-group-item-action">6. Training Loop and Optimisation</a>
          <a href="#validation-and-results" class="list-group-item list-group-item-action">7. Validation and Results</a>
          <a href="#insights-and-learnings" class="list-group-item list-group-item-action">8. Insights and Learnings</a>
          <a href="#conclusion" class="list-group-item list-group-item-action">9. Conclusion</a>
        </div>
      </div>
    </div>
    

    <!-- INTRODUCTION -->
    <section id="introduction" class="mt-5">
      <h2>Introduction</h2>
      <p>In this project, we uncover the rich mathematics that gives rise to modern machine learning and artificial 
        intelligence. Nowadays, artificial intelligence (AI) is being used across a wide range of applications. From large language models (LLMs) 
        such as OpenAI's <em>ChatGPT</em> and Anthropic's <em>Claude</em>, to vision systems in self-driving cars, 
        AI is taking centre stage and affects the lives of us all. In this project, we turn the the complexity down to analyse how seemingly 'dumb' 
        models can make accuracte predicitons when trained on large amounts of data. In this, case we will focus on handwritten 
        digit classification. Given 10s of thousands of training images, we will train a neural network model to learn the 
        underlying structure of digit classification, and validate it to see if it can make accurate predications on 
        unseen data. Better yet, we will forgo modern python frameworks such as PyTorch and TensorFlow to understand the 
        mathematical algorithms and operations involved from scratch.
      </p>
    </section>


    <!-- Handwritten Digits and the MNIST Set -->
    <section id="handwrittenmnist" class="mt-5">
      <h2>Handwritten Digits and the MNIST Set</h2>
      <p>Humans are imperfect. One notable example of this is the way we hand-write digits. Some choose to strikethrough their '7's, some 
        choose to draw '4's in one stroke, while others draw '9's that look like a 'g'. Generally however, the human brain is able to
        process the visual appearance of a variety of differently drawn digits, and classify them, almost instantanously, as belonging to one 
        of the classes 0-9. This is remarkable as no two handwritten digits are the same. One '9' may have a 0.2mm thicker stem than another, 
        while another '6' might have a circle with a radius 0.15mm smaller. With the neural network developed in <a href="#neural-network-architecture">section 3</a>,
        we will attempt to emulate what the human brain does with powerful mathematics and computation.
      </p>
      <p>Any machine learning model requires a large amount of data to learn the complex relationships between inputs and their labelled classes. 
        For this project, we will use the MNIST handwritten digit dataset introduced by LeCun et al. at Bell Labs in 1998. It has the following 
        characteristics
        <ul>
          <li>The MNIST dataset contains 70,000 labeled images of handwritten digits (0–9).</li>
          <li>The set is partitioned into 60,000 training and 10,000 test images.</li>
          <li>Each image is \(28 \times 28\) pixels in grayscale, representing a single digit.</li>
          <li>Pixel values are integers from 0 to 255 representing darkness</li>
          <li>Labels are digits 0-9 as ground truth.</li>
        </ul>
        This dataset is a benchmark widely used in machine learning and computer vision. The current world record for the highest percentage 
        accuracy on the test set is 99.87%, set by the 'Branching/Merging CNN + Homogeneous Vector Capsules' model from the following 
        <a href="https://arxiv.org/abs/2001.09136" target="_blank">paper</a>. Figure 2.1 shows examples of images
        in the test set, alongside their ground-truth labels. Click the button to see more examples.
      </p>
      <figure class="diagram-container text-center mt-3">
        <div class="d-flex justify-content-center align-items-center mb-3" style="gap: 40px;">
          <!-- Canvas: handwritten digit -->
          <canvas id="test-example" width="28" height="28" 
            style="width: 280px; height: 280px; image-rendering: pixelated; border: 1px solid #000; border-radius: 8px;">
          </canvas>

          <!-- Arrow -->
          <div style="font-size: 160px;">→</div>

          <!-- Label -->
          <div class="text-center">
            <div id="image-digit" style="font-size: 250px;">?</div>
          </div>
        </div>

        <button id="random-example" class="btn btn-primary mt-1 mb-4">Show Random Test Image</button>

        <figcaption class="text-muted mt-2">
          Figure 2.1 - Handwritten digit examples from the MNIST test set.
        </figcaption>
      </figure>
      <p>There is clearly much variation between handwritten digits. This mostly comes down to the individual's handwriting legibility, but there 
        are also some cultural differences. For example, the '7' with the strikethrough or the '1' with a hat. Figure 2.2 shows 20 examples of 
        each digit.
      </p>
      <figure class="diagram-container text-center mt-5">
        <img src="assets/nn_digits.png" alt="200 MNIST examples" class="img-fluid w-10 mb-3">

        <figcaption class="text-muted mt-2">
          Figure 2.2 - 200 examples from the MNIST set showing variation between each digit.
        </figcaption>
      </figure>
      <p>Inevitably, given enough humans and written digits there will be handwritten digits so illegible that even a human could not decipher 
      what the intention was. If anything, this reinforces the important of clear writing! Figure 1.3 shows ambiguous examples from the MNIST 
      set, along their questionable labels.
      </p>
      <figure class="diagram-container text-center mt-5">
        <img src="assets/nn_mnist_bad.webp" alt="Bad MNIST digits" class="img-fluid w-70 mb-3">

        <figcaption class="text-muted mt-2">
          Figure 2.3 - Ambiguous examples from the MNIST set.
        </figcaption>
      </figure>
      <p>Keep in mind, statistical machine learning is not perfect and neither is human inference. 
        Nevertheless, <em>most of the time</em> us humans can read handwritten digits with minimal ambiguity. This reliable structure 
        is ultimately what we try to unearth in this project through careful formulation and training of a neural network.
      </p>
    </section>

    <!-- Neural Network Architecture -->
    <section id="#neural-network-architecture" class="mt-5">
      <h2>Neural Network Architecture</h2>
      <p>In this section we introduce the machine learning model used to classify the digits. This, of course, is the 
        neural network. A neural network is a layered mathematical model made up of nodes (neurons) connected by edges (weights). It takes 
        an input (like a grid of pixel values), processes it through one or more hidden layers, and produces an output such as a probability
        distribution over classes. One can think of a neural network as a highly non-linear function which typically maps a large number of 
        inputs to one or more outputs. The non-linearity is what allows the model to mould itself in complex ways and learn the underlying 
        function which maps inputs to their correct labels. We will define what we mean by 'non-linearity' later in this chapter.
        In a typical diagram, such as that in Figure 3.1, you'll see input neurons on the left, hidden layers in the 
        middle, and output neurons on the right, with every connection representing a learned weight. The neural network models how biological
        neurons and synapses (equivalent to weights/edges) work in the brain. Thus, if trained correctly this model should be able to understand 
        complex patterns within spatial image data, just like humans do, to make the correct classification.
      </p>
      <figure class="diagram-container text-center mt-5">
        <img src="assets/nn_netarchit.svg"
            alt="Neural Network Diagram"
            class="img-fluid mb-3"
            style="width: 60%; max-width: 700px; height: auto;">
        <figcaption class="text-muted mt-2">
          Figure 3.1 – A simple neural network.
        </figcaption>
      </figure>
      <p>Each layer of the neural network in Figure 3.1 is explained below.
      </p>
      <ul>
          <li><strong>Input Layer:</strong> On the far left, we see the input neurons. There is no computation inside an input neuron as they corrrespond directly to the 
            <em>features</em> of our input. In the case of the MNIST handwritten digits, which are \(28 \times 28\) pixel images, there are a 
            total of \(28 \times 28 = 784\) input feautures. This means our neural network will require \(784\) input neurons on the input layer. 
            These input features are denoted \(x_i\) and correspond to the dimensions of the input space. This high number of dimensions (i.e. pixels) dictate the granularity 
            and quality of the input. For example, if there were only \(3 \times 3 = 9\) pixels in the image, it is clear that there would not be 
            enough 'room' to distinctively draw the digits '0' to '9'. On the other hand, if we were to take an extremely high resolution like
            \(1,000 \times 1,000 = 1,000,000\) pixels for our images, there would be too much detail in each digit. This additional 
            quality doesn't make the digits any more recognisable. This also, as the reader will later learn, would result in weight matrices with 
            billions of parameters. This would be effectively impossible to train without a large GPU cluster. The \(28 \times 28\) 
            input size provides a balance of not polluting the model with extraneous information, whilst giving enough quality to represent digits
            in an easily recognisable way.
          </li>
          <li><strong>Hidden Layer:</strong> In the centre, we find the hidden layer. A neural network can have many hidden layers, with any number of neurons. Adding more than 
            around three hidden layers results in so-called <em>deep learning</em>. The number of hidden layers and the number of hidden neurons 
            determine the flexibility of the model. The outputs of the hidden neurons are denoted by \(z_{i}^{(n)}\), which denotes the \(i\)-th neuron in 
            the \(n\)-th hidden layer. If we have too few hidden neurons, the model will not be flexible enough. Conversely, if we 
            have too many hidden neurons, our model will learn to rely on the training data too much and effectively start memorising it. Thus, the
            model misses the essence of the problem which is to judge the shapes and contours in each image to determine the most appropriate digit.
            Instead rather, the model will just learn "this is training example 15239 and I remember it's a '9'". Memorisation is not learning. This 
            is so-called <em>overfitting</em>. To combat overfitting, several <em>regularizsation</em> techniques have been establish, such as dropout 
            and max-pooling, which aim to break the fine neural 'memory' structures that result in overfitting.
          </li>
          <li><strong>Output Layer:</strong> On the right, we find the output layer. This layer is responsible for taking the outputs of the final hidden 
            layer and, for the classification problem, producing series of <em>pseudoprobabilities</em> corresponding to the model's confidence 
            that the given input belongs to each class. Hence, the number of output neurons is equal to the number of classes. For our problem
            there will be 10 output neurons, one for each of the digits '0' to '9'. The <em>predication function</em> that we will later define 
            will be responsible for taking the final hidden layer outputs and producing a pseudoprobability strictly in the range of 0 (near 
            impossible) to 1 (near certain). We denote the outputs \(\hat{y_i}\) with a little hat to indicate that they are predications, 
            not the actual ground truth associated with the training example (which is called \(y_i\)).
          </li>
      </ul>
      <p>Now that we understand the structure of the neural network and each of the layers, it is natural to wonder what all the connections (edges) 
        mean and how the network actually processes data. First of all, in a graph nodes may be connected by edges in any way. In the case of a 
        neural network the layers are typically <em>fully connected</em>, meaning every node in one layer connects to every other 
        node in the following layer. The network should also be read from left-to-right. Input features, \(x_i\), come into the model on the left 
        and they <em>forward propogate</em> via the edges to neurons in the following layer, some mathematics is performed, then those outputs follow 
        the same process until we are left with out outputs, \(\hat{y_i}\). The details of that mathematics is given in Figure 3.2.
      </p>
      <figure class="diagram-container text-center mt-5">
        <img src="assets/nn_neuronsum.svg"
            alt="Neural Network Neuron Sum + Activation"
            class="img-fluid mb-3"
            style="width: 100%; max-width: 900px; height: auto;">
        <figcaption class="text-muted mt-2">
          Figure 3.2 – Flow diagram showing weighted sum and activation used to produce hidden neuron output \(z_{1}^{(1)}\)
        </figcaption>
      </figure>
      <p>Figure 3.2 is actually a truncated version of Figure 3.1, showing only the input neurons and the first neuron in the 1st (and only) hidden 
        layer. The output of this neuron is \(z_{1}^{(1)}\). Following the diagram, the first step is to take the weighted sum of each input \(x_i\) 
        and the weight corresponding to edge between the origin and destination node. The destination node in this case is the first node in the 
        hidden layer, so the first index in each weight is '1'. After the sum we also add the bias term \(b_{1}^{(i)}\). The bias allows the model to 
        be even more flexible by introducing an offset which is independent of the input values. Think about it as the '\(b\)' term in the linear function 
        \(y = mx + b\). Following that, we feed the weighted sum plus bias into a non-linear <em>activation function</em>. The output of this is the 
        familiar \(z_{1}^{(1)}\) from Figure 3.1. Whilst Figure 3.2 describes what happens for \(z_{1}^{(1)}\) only, the other \(z_{i}^{(n)}\) follow 
        the exact same process, with input data always coming from the previous layer. The exception to this is the output layer, which operates the 
        same, except with a different activation function.
      </p>
      <p>With the neural network structure defined and the feed forward process intuitively understood, we now motivate the selection 
        of various functions which give the network properties we like. These are the <em>loss function</em>, <em>prediction function</em> and
        the <em>activation function</em>.
      </p>

      <h3 class="mt-5">Acitvation Function</h3>
        <p>The non-linear activation step is paramount in allowing the network to create the high levels of flexibility required to learn 
          complex relationships. In fact, when we start discussing the linear algebra behind these operations in <a href="#forward-propagation">section 4</a>, 
          we will see that if no activation function is present, or it is linear, the effect of subsequent layers is nil and the model's output 
          will simply be a linear combination of the inputs. Besides being non-linear 
        </p>
      
      <h3 class="mt-5">Prediction Function</h3>
      <p>As alluded earlier, the predication function is a special activation function which is only used on the ouput layer of the network 
        to pseudoprobabilities for the outputs \(y_{i}\). These outputs correspond to each of the possible classes and are called 
        <em>logits</em>.
      </p>

    </section>


    <!-- SIMULATION -->
    <section id="simulation" class="mt-5">
      <h2>Simulation</h2>
      <p>
        We sim
      </p>

      
    </section>

    <section id="pcb-design" class="mt-5 mb-5">
      <h2>PCB Design with KiCAD 8</h2>
      <p>
        Onno
      </p>
      <h3 class="mt-5">6.1 Schematic Capture</h3>
      <p>
        Deity
      </p>

      <h3 class="mt-5">6.2 PCB Layout & Routing</h3>
      <p>
        Structural
      </p>
      

      <h3 class="mt-5">6.3 Assembly and Testing</h3>
      <p>
        A week later, the training finished on the swarm of 99,000 NVIDIA 4090 GPUs
      </p>
      
      
    </section>

    <section id="finished-design" class="mt-5">
      <h2>Finished Design</h2>
      <p>
        Fin des
      </p>

    </section>

    <section id="conclusion" class="mt-5">
      <h2>Conclusion</h2>
      <p>
        NNs are dope
      </p>
    </section>
  </main>

  <!-- Back-to-Top -->
  <div class="text-center my-5">
    <button id="backToTop" class="btn btn-primary">Back to Top</button>
  </div>

  <!-- Footer -->
  <footer class="text-center text-lg-start bg-light text-muted" style="background-color: #e3f2fd; padding:1px; bottom:0; width:100%;">
    <section class="d-flex justify-content-between align-items-center p-4">
      <div class="me-5 d-none d-lg-block">
        <span>&copy; 2025 Lachlan Cooke</span>
      </div>
      <div class="social-menu ms-auto">
        <ul class="d-flex">
          <li class="ms-3"><a href="https://www.linkedin.com/in/lachlancooke/" target="_blank"><i class="fab fa-linkedin-in"></i></a></li>
          <li class="ms-3"><a href="https://github.com/coolachlanke" target="_blank"><i class="fab fa-github"></i></a></li>
          <li class="ms-3"><a href="https://www.instagram.com/coolachlanke/" target="_blank"><i class="fab fa-instagram"></i></a></li>
          <li class="ms-3"><a href="https://www.youtube.com/@magicbattery" target="_blank"><i class="fab fa-youtube"></i></a></li>
          <li class="ms-3"><a href="https://www.strava.com/athletes/30032118" target="_blank"><i class="fab fa-strava"></i></a></li>
        </ul>
      </div>
    </section>
  </footer>

  <!-- JS libs -->
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.9.11/dist/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-python.min.js"></script>
  <script src="https://cdn.jsdelivr.net/pyodide/v0.25.1/full/pyodide.js"></script>


  <!-- Custom JS -->
  <script src="neuralnet.js"></script>          <!-- UI glue + canvas -->
  <script src="mnist_inference.js"></script>    <!-- Math + model -->

</body>
</html>
